{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASK-Maintaining Data Privacy in Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_ds_path = \".\\..\\Datasets\\store_data.csv\"\n",
    "\n",
    "with open(test_ds_path, \"r\") as f:\n",
    "\n",
    "    lines = f.readlines()\n",
    "    inventory = list()\n",
    "    tuples = list()\n",
    "\n",
    "    for line in lines:\n",
    "        transaction = line.strip().split(\",\")\n",
    "        #print(transaction)\n",
    "        tuples.append(transaction)\n",
    "        for element in transaction:\n",
    "            if element.strip().replace(' ','_').replace('&','and') not in inventory:\n",
    "                inventory.append(element.strip().replace(' ','_').replace('&','and'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(inventory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(inventory)\n",
    "inventory.pop()\n",
    "inventory.sort()\n",
    "print(inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(inventory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.DataFrame(\n",
    "    [[1 if item in row else 0 for item in inventory] for row in tuples],\n",
    "    columns=inventory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['zucchini'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tuple in test_dataset.itertuples():\n",
    "    print(tuple)\n",
    "    print(int( getattr(tuple, 'almonds')))\n",
    "    print(type(tuple))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for tuple in test_dataset.itertuples(False):\n",
    "    print(tuple)\n",
    "    test_tuple = tuple\n",
    "    print(type(tuple))\n",
    "    print(np.asarray(tuple))\n",
    "    break\n",
    "\n",
    "dataset_mini = test_dataset.head(3)\n",
    "\n",
    "\n",
    "print(dataset_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mini.loc[0,'almonds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def support(T: DataFrame ,X_U_Y: list | dict):\n",
    "    '''\n",
    "    Parameters:\n",
    "    T (dataframe)\n",
    "    X_U_Y (list | dict) : name of the attributes considered (X U Y)\n",
    "    Return:\n",
    "    float: support of the attributes in the dataset\n",
    "    '''\n",
    "    for attribute in X_U_Y:\n",
    "        if attribute not in T.columns:\n",
    "             return ValueError\n",
    "    count = 0\n",
    "    for tuple in T.itertuples(False):\n",
    "        contained = True\n",
    "        for attribute in X_U_Y:\n",
    "            try: \n",
    "                if getattr(tuple,attribute) == 0:\n",
    "                    contained = False\n",
    "                    break\n",
    "            except AttributeError:\n",
    "                contained = False,\n",
    "                break\n",
    "\n",
    "        if contained:\n",
    "            count += 1 \n",
    "    return count/len(T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support(test_dataset,['herb_and_pepper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def confidence(T: DataFrame, X: list | dict, Y: list | dict):\n",
    "    '''\n",
    "    Parameters:\n",
    "    T (dataframe)\n",
    "    X ( list | dict): X part of X ==> Y rule\n",
    "    X (list | disct): Y part of X ==> Y rule\n",
    "    '''\n",
    "    if len(X)+len(Y) > len(T.columns):\n",
    "        return ValueError\n",
    "    for item in X:\n",
    "        if Y.__contains__(item):\n",
    "            return ValueError # XY = empty set\n",
    "    \n",
    "    countX=0\n",
    "    countY=0\n",
    "    for tuple in T.itertuples():\n",
    "        containedX = True\n",
    "        for attribute in X:\n",
    "            if getattr(tuple,attribute) == 0:\n",
    "                containedX = False\n",
    "                break\n",
    "        if containedX:\n",
    "            countX += 1\n",
    "            containedY=True\n",
    "            for attribute in Y:\n",
    "                if attribute == '':\n",
    "                    containedY = False\n",
    "                    break\n",
    "                if getattr(tuple,attribute) == 0:\n",
    "                    containedY = False\n",
    "                    break\n",
    "            if containedY:\n",
    "                countY += 1\n",
    "    return countY/countX    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence(test_dataset,['spaghetti','champagne'],['cookies'])\n",
    "#spaghetti,champagne,cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Apriori(items, dataset, min_sup, levels: int = None):\n",
    "    if levels is None:\n",
    "        levels = len(items)\n",
    "    '''\n",
    "    rules[0] = empty set\n",
    "    rules[1] = rules of length 1\n",
    "    rules[2] = rules of length 2\n",
    "\n",
    "\n",
    "    rules[len(items)] = items\n",
    "    ...\n",
    "    '''\n",
    "    rules = [\n",
    "        [],\n",
    "        [ [item] for item in items ]\n",
    "    ]\n",
    "\n",
    "\n",
    "    # iterate over all the possible rules length from 1 to len(items)\n",
    "    for i in range(1, levels+1):\n",
    "        print(f\"Apriori Level: {i}\")\n",
    "        # remove all the rules in rules[i]\n",
    "        # that don't have a support of at least min_sup\n",
    "\n",
    "        #print(f\"RULES[{i}] BEFORE\", rules[i])\n",
    "\n",
    "        rules[i] = [\n",
    "            rule\n",
    "            for rule in rules[i]\n",
    "            if support(dataset, rule) >= min_sup\n",
    "        ]\n",
    "        \n",
    "        '''\n",
    "        print(f\"RULES[{i}] SUPPORT\", [\n",
    "            support(dataset, rule)\n",
    "            for rule in rules[i]\n",
    "        ]\n",
    "        )'''\n",
    "\n",
    "\n",
    "        #print(f\"RULES[{i}] AFTER\", rules[i])\n",
    "\n",
    "        if len(rules[i]) == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "        # generate all the possibile \n",
    "        # rules with i+1 elements\n",
    "        rules.append([]) # create the element rules[i+1]\n",
    "\n",
    "        #print(\"RULE[i]\", rules[i])\n",
    "        #print(\"RULE[i+1]\", rules[i+1])\n",
    "        for rule in rules[i]:\n",
    "            for item in items:\n",
    "\n",
    "                # skip if item is already inside the rule\n",
    "                if item in rule:\n",
    "                    continue\n",
    "\n",
    "                rules[i+1].append ( rule + [item] )\n",
    "                # create a new rule composed of rule + [item]\n",
    "\n",
    "\n",
    "        #print(\"RULE[i+1]\", rules[i+1])\n",
    "    \n",
    "        #print()\n",
    "        #print()\n",
    "        #print()\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#rules = Apriori(inventory,test_dataset,0.004, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector = [[item,support(test_dataset,[item])] for item in inventory]\n",
    "print(support_vector)\n",
    "mean_support = np.mean([sublist[1] for sublist in support_vector])\n",
    "print(mean_support)\n",
    "\n",
    "frequent_item = []\n",
    "infrequent_item = []\n",
    "\n",
    "\n",
    "for item in support_vector:\n",
    "    if item[1]>mean_support:\n",
    "        frequent_item.append(item)\n",
    "    else:\n",
    "        infrequent_item.append(item)\n",
    "\n",
    "\n",
    "\n",
    "support_vector\n",
    "print(support_vector)\n",
    "print(frequent_item)\n",
    "print(infrequent_item)\n",
    "\n",
    "print(len(support_vector))\n",
    "print(len(frequent_item))\n",
    "print(len(infrequent_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def MASK_Distortion(dataset: DataFrame, p: float):\n",
    "    '''\n",
    "    MASK\n",
    "    \n",
    "    choose a probability p\n",
    "\n",
    "    2 event\n",
    "    P(x = true) -> 1-p % -> we add or remove an element in the transaction\n",
    "    P(x = false) -> p% -> the transaction remains the same\n",
    "    '''\n",
    "    distorted_dataset = dataset.copy(deep=True)\n",
    "    for i in range(0,len(distorted_dataset)):\n",
    "        for column in distorted_dataset.columns:\n",
    "            event = random.random()\n",
    "            if event > p:\n",
    "                distorted_dataset.loc[i, column] = int(not dataset.loc[i, column])\n",
    "\n",
    "    return distorted_dataset\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MASK_frequent_infrequent_items(support_vector,sup_min):\n",
    "    frequent_item = []\n",
    "    infrequent_item = []\n",
    "\n",
    "    for item in support_vector:\n",
    "        if item[1]>sup_min:\n",
    "            frequent_item.append(item[0])\n",
    "        else:\n",
    "            infrequent_item.append(item[0])\n",
    "    return frequent_item,infrequent_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.9\n",
    "distorted_test_dataset = MASK_Distortion(test_dataset,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hammingDistanceBitwise(a:int,b:int):\n",
    "    return (a^b).bit_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hammingDistanceBitwise(int(1),int(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "M = np.diag([math.pow(2,3) for i in range(0,3)])\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def computeM(size: int, p: float):\n",
    "    max_exp = int(math.log2(size))\n",
    "    M = np.diag([math.pow(p,max_exp) for i in range(size)])\n",
    "    for i in range(0,size):\n",
    "        for j in range(i+1,size):\n",
    "            difference = hammingDistanceBitwise(i,j)\n",
    "            M[i][j] = math.pow(1-p,difference)*math.pow(p,max_exp-difference)\n",
    "\n",
    "    for i in range(1,size):\n",
    "        for j in range(i,-1,-1):\n",
    "            M[i][j] = M[j][i]\n",
    "\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_test = computeM(int(math.pow(2,2)),0.1)\n",
    "print(M_test)\n",
    "print(sum(M_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLinC_D(dataset: DataFrame,rule: list | dict):\n",
    "    linC_D = np.zeros(len(rule)+1,dtype=int)\n",
    "    reducted_ds = dataset[rule] # posso prendere solo le colonne interessanti\n",
    "    for tuple in reducted_ds.itertuples(False):\n",
    "        '''sum = 0\n",
    "        for attribute in rule:\n",
    "            sum += getattr(tuple,attribute) # o farlo qua con il for\n",
    "            linC_D[sum] += 1\n",
    "        '''\n",
    "        \n",
    "        linC_D[np.sum(tuple,axis=0)] += 1\n",
    "    \n",
    "    return linC_D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectormatrixProdMod(linC_D,matrix):\n",
    "    size = matrix.shape[0]\n",
    "    row = matrix[size-1]\n",
    "    sum=0\n",
    "    for j in range(len(linC_D)):\n",
    "        index = int(math.pow(2,j))-1\n",
    "        #print(str(row[index])+\" * \"+str(linC_D[j]))\n",
    "        sum += row[index]*linC_D[j]\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "M_test = inv(computeM(2**2,p))\n",
    "print(M_test)\n",
    "print(sum(M_test[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def MASK_Support(dataset:DataFrame, rule, p:float, M_inv = None):\n",
    "    if M_inv is None:\n",
    "        M_inv = inv(\n",
    "            computeM(\n",
    "                size=int(math.pow(2,len(rule))),\n",
    "                p=p\n",
    "            )\n",
    "        )\n",
    "\n",
    "    linC_D = computeLinC_D(dataset,rule)\n",
    "\n",
    "    #print(linC_D)\n",
    "    #print(M_inv)\n",
    "\n",
    "    C_T_11 = vectormatrixProdMod(linC_D,M_inv)\n",
    "\n",
    "    #print(C_T_11)\n",
    "\n",
    "    return C_T_11/len(dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "def MASK_Apriori(dataset: DataFrame, p: float, min_sup,levels: int = None):\n",
    "    if levels is None:\n",
    "        levels = len(dataset.columns)\n",
    "    rules = [\n",
    "        [],\n",
    "        [ [item] for item in dataset.columns ]\n",
    "    ]\n",
    "\n",
    "    for i in range(2, levels+1):\n",
    "        print(f\"Constructing Level: {i}\")\n",
    "        combinations = list(itertools.combinations(dataset.columns,i))\n",
    "        rule_i = [list(c) for c in combinations]\n",
    "        rules.append(rule_i)\n",
    "    \n",
    "\n",
    "                    # create a new rule composed of rule + [item]\n",
    "        \n",
    "\n",
    "    for i in range(1,levels+1):\n",
    "        print(f\"Analysing Level: {i}\")\n",
    "        size = int(math.pow(2,i))\n",
    "        M_inv = inv(\n",
    "            computeM(size,p)\n",
    "        )\n",
    "        \n",
    "        rules[i] = [\n",
    "            rule\n",
    "            for rule in rules[i]\n",
    "            if MASK_Support(dataset,rule,p,M_inv) >= min_sup\n",
    "        ]\n",
    "\n",
    "    return rules\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_inventory = inventory[0:5]\n",
    "print(reduced_inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_test_ds = test_dataset.iloc[0:1000, 0:5]\n",
    "print(reduced_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_reduced = [[item,support(reduced_test_ds,[item])] for item in reduced_inventory]\n",
    "print(support_vector_reduced)\n",
    "mean_reduced_support = np.mean([sublist[1] for sublist in support_vector_reduced])\n",
    "print(mean_reduced_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.9\n",
    "reduced_distorted_test_ds = MASK_Distortion(reduced_test_ds,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.001\n",
    "rules = Apriori(reduced_inventory,reduced_test_ds,min_sup)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_rules = MASK_Apriori(reduced_distorted_test_ds,p,min_sup)\n",
    "print(MASK_rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
